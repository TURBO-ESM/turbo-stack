name: Build Inside NCAR HPC Development Containers

# See the following link for more information on the NCAR HPC development containers:
# https://hub.docker.com/u/ncarcisl

on:
  workflow_dispatch:
  push:
    branches: ["main", "ci-tests", "container-ci"]
  pull_request:
    branches: ["main", "ci-tests", "container-ci"]

jobs:

  run-matrix:
    strategy:
      fail-fast: false
      matrix:
        #compiler: [ nvhpc, oneapi, gcc14, clang ]
        compiler: [ gcc14, clang ] # temporarily test one compiler at a time. Add all compilers back later.
        mpi:      [ openmpi, mpich ]
        #mpi:      [ openmpi ] # temporarily test one mpi at a time. Add all back later.
        #gpu:      [ nogpu, cuda ]
        gpu:      [ nogpu ]
        arch:     [ x86_64 ]
        #runner:   [ ubuntu-latest, gha-runner-turbo ]
        runner:   [ gha-runner-turbo ]
        include:
          - mpi: openmpi
            extra_mpiexec_args: '--allow-run-as-root'

        exclude:
          # skip nvhpc/openmpi until CPU architecture mismatch gets resolved
          - compiler: nvhpc
            mpi: openmpi


    name: Build
    runs-on: ${{ matrix.runner }}
    defaults:
      run:
        shell: bash -elo pipefail {0}

    container:
      image: ncarcisl/cisldev-${{ matrix.arch }}-almalinux9-${{ matrix.compiler }}-${{ matrix.mpi }}${{ matrix.gpu == 'cuda' && '-cuda' || '' }}

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
          fetch-depth: 0

      - name: Interrogate Runtime Environment
        run: |
          cat /container/config_env.sh
          lscpu
          echo && echo && echo
          echo '----------------------------------------------------------------'
          echo && echo && echo
          echo "CC=${CC}"
          echo "CXX=${CXX}"
          echo "FC=${FC}"
          echo "F77=${F77}"
          echo
          echo "CFLAGS=${CFLAGS}"
          echo "CPPFLAGS=${CPPFLAGS}"
          echo "CXXFLAGS=${CXXFLAGS}"
          echo "FCFLAGS=${FCFLAGS}"
          echo "F77FLAGS=${F77FLAGS}"
          export CC CXX FC F77 CFLAGS CXXFLAGS FCFLAGS F77FLAGS CPPFLAGS
          conda --version 2>/dev/null || echo " --> no conda in this container"
          which mpicc
          mpicc --version 2>/dev/null || true

      - name: MPI+OpenMP Hello World
        run: |
          mpicxx -o ./hello_world_mpi /container/extras/hello_world_mpi.C -fopenmp
          ldd ./hello_world_mpi
          export OMP_NUM_THREADS=2
          mpiexec -n 2 ${{ matrix.extra_mpiexec_args }} ./hello_world_mpi

          if [[ "${MPI_FAMILY}" == "openmpi" ]]; then
              mpicxx --showme
          elif [[ "${MPI_FAMILY}" == "mpich" ]]; then
              mpicxx -show
          fi

# Temporarily disable turbo-stack and double gyre test
#      - name: Build turbo-stack
#        run: |
#          echo "Building turbo-stack..."
#          ./build.sh --machine container --compiler ${COMPILER_FAMILY}
#
#      - name: Run double gyre test
#        run: |
#          echo "Running double gyre test..."
#          cd ./examples/double_gyre
#          mpiexec -n 2 ${{ matrix.extra_mpiexec_args }} ../../bin/${COMPILER_FAMILY}/MOM6/MOM6

      - name: Install prerequisite utilities for spack
        run: |
          dnf install -y epel-release
          dnf install -y file bzip2 ca-certificates git gzip patch python3 tar unzip xz zstd gcc gcc-c++ gcc-gfortran

      - name: Install spack
        run: |
          echo "Building spack:"
          git clone --depth=2 https://github.com/spack/spack.git
          . spack/share/spack/setup-env.sh
          spack compiler find
          spack compiler list
          spack spec hdf5

      - name: Build and run amrex mini-app
        run: |

          # get spack command in path
          . spack/share/spack/setup-env.sh

          echo "Building spack environment for turbo mini-app..."
          export TURBO_STACK_ROOT=$(pwd)
          echo "TURBO_STACK_ROOT=${TURBO_STACK_ROOT}"
          DEBUG=1 ./src/development_tests/amrex/tripolar_grid/build_and_run.sh
